{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel corpus preprocess\n",
    "This notebook takes as input some snorkel-compatible files (corpus, entities, gold relation labels) and creates snorkel.db, where all of the above are persisted.\n",
    "re-run to drop and re-create db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonis/anaconda2/envs/snorkel27clone/lib/python2.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created snorkel session from  postgres:///snorkel25similar\n"
     ]
    }
   ],
   "source": [
    "experiment_name = '25similar'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%run init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with SQLite instead\n",
    "# from snorkel import SnorkelSession\n",
    "# session = SnorkelSession()\n",
    "# from snorkel.models import  Document, Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for debugging\n",
    "from utils import get_raw_document_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40743 documents\n"
     ]
    }
   ],
   "source": [
    "# Get train,dev,test from goldset, and chemdner_silver from NCBI_parsed (ensure consistency of named entities)\n",
    "txt_corpus = glob.glob(\"/home/antonis/data/biocreative6/NCBI_parsed/similar25/*.txt\") + \\\n",
    "                glob.glob(\"/home/antonis/data/biocreative6/goldset/*/*.txt\")\n",
    "\n",
    "# test cand extr+ goldlabel generator\n",
    "# txt_corpus = glob.glob(\"/home/antonis/data/biocreative6/goldset/*/*.txt\")\n",
    "\n",
    "txt_corpus = pd.Series(txt_corpus,name='paths')\n",
    "print len(txt_corpus), 'documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_corpus_paths.to_csv('full_corpus_paths.csv',header=True)\n",
    "txt_corpus.to_csv('full_corpus_paths.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import TextDocPreprocessor, CSVPathsPreprocessor\n",
    "# path = \"/home/antonis/data/biocreative6/corpus/training/\"\n",
    "# doc_preprocessor = TextDocPreprocessor(path)\n",
    "\n",
    "csv_preprocessor = CSVPathsPreprocessor('full_corpus_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Documents:', 40743L)\n",
      "('Sentences:', 448455L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.utils_cdr import TaggerOneTagger, CDRTagger\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "\n",
    "\n",
    "tagger_one = TaggerOneTagger(fname_tags=\n",
    "                             '/home/antonis/data/biocreative6/entities/unary_tags.pkl.bz2',\n",
    "                            fname_mesh=\n",
    "                             '/home/antonis/data/biocreative6/entities/mesh_dict.pkl.bz2')\n",
    "\n",
    "corpus_parser = CorpusParser(parser = Spacy() , fn = tagger_one.tag)\n",
    "\n",
    "corpus_parser.apply(list(csv_preprocessor), parallelism=6)\n",
    "\n",
    "# Inspect DB contents\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** DebuG ** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DebuG\n",
    "# print(\"sample docs inserted: \")\n",
    "# map(lambda x: x[1],csv_preprocessor)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # use Spacy as an alternative for now\n",
    "# corpus_parser = CorpusParser( parser = Spacy() )\n",
    "# corpus_parser.apply(doc_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Inspect given entities in sentence\n",
    "# for i in range(10):\n",
    "#     print ','.join(session.query(Sentence).all()[i].entity_types)\n",
    "#     print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_pubmed_ids_pkl, 'rb') as f:\n",
    "    pubmed_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['incoming_citations',\n",
       " 'test_gs',\n",
       " 'outgoing_citations',\n",
       " 'train',\n",
       " 'similar25',\n",
       " 'validation']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed_ids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids  =  set(pubmed_ids['train'])\n",
    "val_ids = set(pubmed_ids['validation'])\n",
    "test_ids = set(pubmed_ids['test_gs'])\n",
    "# chemdner_silver_ids =  set(pubmed_ids['chemdner_silver'])\n",
    "unlab_ids = set(pubmed_ids['similar25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, val_sents, test_sents, unlab_sents = set(), set(), set(), set()\n",
    "docs = session.query(Document).order_by(Document.name).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)            \n",
    "        elif doc.name in val_ids:\n",
    "            val_sents.add(s)\n",
    "        elif doc.name in test_ids:\n",
    "            test_sents.add(s)\n",
    "        elif doc.name in unlab_ids:\n",
    "            unlab_sents.add(s)\n",
    "        else:\n",
    "            raise Exception('ID <{0}> not found in any id set'.format(doc.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved into init.py\n",
    "# from snorkel.models import Candidate, candidate_subclass\n",
    "# REGULATOR = candidate_subclass('REGULATOR', ['Chemical', 'Gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import PretaggedCandidateExtractor\n",
    "candidate_extractor = PretaggedCandidateExtractor(REGULATOR, ['Chemical', 'Gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13542\n",
      "3067\n",
      "8171\n",
      "423675\n"
     ]
    }
   ],
   "source": [
    "for k, sents in enumerate([train_sents, val_sents, test_sents, unlab_sents]):\n",
    "    print len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 21184L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 5080L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 13935L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 130424L)\n"
     ]
    }
   ],
   "source": [
    "for k, sents in enumerate([train_sents, val_sents, test_sents, unlab_sents]):\n",
    "    candidate_extractor.apply(sents, split=k, parallelism=6)\n",
    "    print(\"Number of candidates:\", session.query(REGULATOR).filter(REGULATOR.split == k).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_external_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 13007\n",
      "AnnotatorLabels not matched to candidates (split=0): 308536\n",
      "AnnotatorLabels created: 3242\n",
      "AnnotatorLabels not matched to candidates (split=1): 318301\n",
      "AnnotatorLabels created: 8221\n",
      "AnnotatorLabels not matched to candidates (split=2): 313322\n"
     ]
    }
   ],
   "source": [
    "from snorkel.db_helpers import reload_annotator_labels\n",
    "# load_external_labels(session,\n",
    "#                      REGULATOR,\n",
    "#                      FPATH='../../data/biocreative6/gold_rels_snorkel_format.tsv'\n",
    "# #                      id_fname='../../data/biocreative6/pubmed_ids_extended.pickle'\n",
    "#                     )\n",
    "\n",
    "#load external labels into db\n",
    "load_external_labels(session, REGULATOR, tsv_path='/home/antonis/data/biocreative6/entities/gold_rels_complete.tsv', \n",
    "                     reload=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whats going on with candidate mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import StableLabel\n",
    "from sqlalchemy import and_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split =  0\n",
      "Total cands: 21184\n",
      "Mapped cands: 13007\n",
      "Un-mapped cands: 8177\n",
      "True\n",
      "\n",
      "split =  1\n",
      "Total cands: 5080\n",
      "Mapped cands: 3242\n",
      "Un-mapped cands: 1838\n",
      "True\n",
      "\n",
      "split =  2\n",
      "Total cands: 13935\n",
      "Mapped cands: 8221\n",
      "Un-mapped cands: 5714\n",
      "True\n",
      "\n",
      "split =  3\n",
      "Total cands: 130424\n",
      "Mapped cands: 0\n",
      "Un-mapped cands: 130424\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(4):\n",
    "    print 'split = ',k\n",
    "    print 'Total cands:', session.query(REGULATOR).filter(REGULATOR.split == k).count()\n",
    "    print 'Mapped cands:', session.query(REGULATOR).filter(REGULATOR.split == k).filter(REGULATOR.gold_labels).count()\n",
    "    print 'Un-mapped cands:', session.query(REGULATOR).filter(REGULATOR.split == k).filter(~REGULATOR.gold_labels.any()).count()\n",
    "    print session.query(REGULATOR).filter(REGULATOR.split == k).count() == (session.query(REGULATOR).filter(REGULATOR.split == k).filter(REGULATOR.gold_labels).count() +\n",
    "                                                                           session.query(REGULATOR).filter(REGULATOR.split == k).filter(~REGULATOR.gold_labels.any()).count())\n",
    "    print ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 8177 candidates from split=0 in to_drop list\n",
      "Adding 1838 candidates from split=1 in to_drop list\n",
      "Adding 5714 candidates from split=2 in to_drop list\n"
     ]
    }
   ],
   "source": [
    "#create list of unmapped cands to drop\n",
    "to_drop = []\n",
    "for k in range(3):\n",
    "    query = session.query(REGULATOR).filter(and_(REGULATOR.split==k,~REGULATOR.gold_labels.any()))\n",
    "    print 'Adding %i candidates from split=%i in to_drop list'%(query.count(), k)\n",
    "    to_drop.extend(map(lambda x: x.id,query.all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~~~~ STOPPED HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15729"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop unmapped cands\n",
    "query = session.query(Candidate).filter(Candidate.id.in_(to_drop))\n",
    "print query.count()\n",
    "query.delete(synchronize_session=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm they were dropped\n",
    "query.count() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split =  0\n",
      "Total cands: 13007\n",
      "Mapped cands: 13007\n",
      "Un-mapped cands: 0\n",
      "True\n",
      "\n",
      "split =  1\n",
      "Total cands: 3242\n",
      "Mapped cands: 3242\n",
      "Un-mapped cands: 0\n",
      "True\n",
      "\n",
      "split =  2\n",
      "Total cands: 8221\n",
      "Mapped cands: 8221\n",
      "Un-mapped cands: 0\n",
      "True\n",
      "\n",
      "split =  3\n",
      "Total cands: 130424\n",
      "Mapped cands: 0\n",
      "Un-mapped cands: 130424\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(4):\n",
    "    print 'split = ',k\n",
    "    print 'Total cands:', session.query(REGULATOR).filter(REGULATOR.split == k).count()\n",
    "    print 'Mapped cands:', session.query(REGULATOR).filter(REGULATOR.split == k).filter(REGULATOR.gold_labels).count()\n",
    "    print 'Un-mapped cands:', session.query(REGULATOR).filter(REGULATOR.split == k).filter(~REGULATOR.gold_labels.any()).count()\n",
    "    print session.query(REGULATOR).filter(REGULATOR.split == k).count() == (session.query(REGULATOR).filter(REGULATOR.split == k).filter(REGULATOR.gold_labels).count() +\n",
    "                                                                           session.query(REGULATOR).filter(REGULATOR.split == k).filter(~REGULATOR.gold_labels.any()).count())\n",
    "    print ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting candidates from snorkel to sklearn for ML model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_bridge import export_snorkel_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 13007 candidates from split = 0 \n",
      "Extracted 3242 candidates from split = 1 \n",
      "Extracted 8221 candidates from split = 2 \n",
      "Extracted 130424 candidates from split = 3 \n",
      "Extracted 154894 candidates in total\n"
     ]
    }
   ],
   "source": [
    "# export candidates for train, dev, test dataset\n",
    "candidates = dict()\n",
    "nr_cands_extracted=0\n",
    "for i in range(4): #for train,dev,test export only labelled candidates \n",
    "    candidates[i] = export_snorkel_candidates(session,REGULATOR, i, True)\n",
    "    print 'Extracted %i candidates from split = %i '%(len(candidates[i].keys()), i)\n",
    "    nr_cands_extracted += len(candidates[i].keys())\n",
    "\n",
    "print 'Extracted %i candidates in total'%nr_cands_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_candidate_dict_pkl, 'wb') as f:\n",
    "    pickle.dump(dict(candidates),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #########################################\n",
    "# Once this is done, results are persisted into snorkel.db and this step is no longer required, unless more documents are added.\n",
    "# #########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snorkel27clone]",
   "language": "python",
   "name": "conda-env-snorkel27clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
