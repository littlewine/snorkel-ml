{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild text\n",
    "from sklearn_bridge import recreate_text_representation\n",
    "# take text + labels from train ds\n",
    "from sklearn_bridge import candidate_dict_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLutils import report_to_df, train_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, time, seaborn, codecs\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split = 0 : imported 12987 candidates\n",
      "Split = 1 : imported 3230 candidates\n",
      "Split = 2 : imported 8335 candidates\n",
      "Split = 3 : imported 79400 candidates\n"
     ]
    }
   ],
   "source": [
    "with open('candidates_TrainValTestOutgoing.pickle', 'rb') as f:\n",
    "    candidate_dict = pickle.load(f)\n",
    "\n",
    "for splt in candidate_dict.keys():\n",
    "    print \"Split = %i : imported %i candidates\" %(splt, len(candidate_dict[splt].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train, dev, test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trim_text = True\n",
    "trim_window = 5\n",
    "use_lemmas = True\n",
    "\n",
    "df_train = candidate_dict_to_df(candidate_dict[0],trim_text=trim_text, window=trim_window, lemmas = use_lemmas )\n",
    "df_val = candidate_dict_to_df(candidate_dict[1],trim_text=trim_text, window=trim_window, lemmas = use_lemmas )\n",
    "df_test = candidate_dict_to_df(candidate_dict[2],trim_text=trim_text, window=trim_window, lemmas = use_lemmas )\n",
    "\n",
    "df_unlab = candidate_dict_to_df(candidate_dict[3],trim_text=trim_text, window=trim_window, lemmas = use_lemmas )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_breaks(df):\n",
    "    df['text'] = map(lambda x: x.replace('\\n', ' ' ),df.text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_and_get_ids(df, path):\n",
    "    \"\"\"Splits data into positive and negative examples and returns a list of all the ids seperated\"\"\"\n",
    "    df = remove_line_breaks(df)\n",
    "    \n",
    "    if df.label.any(): #means we have labels\n",
    "        df_pos = df[df.label==1]\n",
    "        df_neg = df[df.label==-1]\n",
    "\n",
    "        df_pos.to_csv(path+'.pos',header=False,index= False,encoding='utf-8', columns=[ 'text'],\n",
    "                   sep= ' ')\n",
    "        df_neg.to_csv(path+'.neg',header=False,index= False,encoding='utf-8', columns=[ 'text'],\n",
    "                   sep= ' ')\n",
    "        \n",
    "        #keep the order of indices after reshuffling\n",
    "        ids = list(df_pos.index)\n",
    "        ids = ids + list(df_neg.index)\n",
    "        return ids\n",
    "    else:\n",
    "        \n",
    "        df.to_csv(path+'.nolabels',header=False,index= False,encoding='utf-8', columns=[ 'text'],\n",
    "                   sep= ' ')\n",
    "        return list(df.index)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = save_dataset_and_get_ids(df_train, 'cnn/df_train')\n",
    "\n",
    "val_ids = save_dataset_and_get_ids(df_val, 'cnn/df_val')\n",
    "\n",
    "test_ids = save_dataset_and_get_ids(df_test, 'cnn/df_test')\n",
    "\n",
    "unlab_ids = save_dataset_and_get_ids(df_unlab, 'cnn/df_unlab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "\n",
    "\n",
    "# save_pos_neg_df(df_train, 'cnn/df_train')\n",
    "# train_ids = list(df_train.index)\n",
    "# save_pos_neg_df(df_val, 'cnn/df_val')\n",
    "# val_ids = list(df_val.index)\n",
    "# save_pos_neg_df(df_test, 'cnn/df_test')\n",
    "# test_ids = list(df_test.index)\n",
    "# save_pos_neg_df(df_unlab, 'cnn/df_unlab')\n",
    "# unlab_ids = list(df_unlab.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** data_helpers.py **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably not needed, can be loaded directly as df / matrix later\n",
    "\n",
    "\n",
    "def load_data_and_labels(pos_file, neg_file=None):\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(codecs.open(pos_file, \"r\", encoding='utf-8' ).readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    if neg_file:\n",
    "        negative_examples = list(codecs.open(neg_file, \"r\", encoding='utf-8').readlines())\n",
    "        negative_examples = [s.strip() for s in negative_examples]\n",
    "        x_text = positive_examples + negative_examples\n",
    "    else:\n",
    "        negative_examples = None\n",
    "        x_text = positive_examples\n",
    "        \n",
    "    # Split by words\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    if neg_file:\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "        return [x_text, y]\n",
    "    else:\n",
    "        return x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"<PAD/>\", sequence_length = 64):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "#     sequence_length = max(len(x) for x in sentences)    #commented this out to avoid problems with building in new training data\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = max(0,sequence_length - len(sentence))\n",
    "        if num_padding>0:\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "        else: #crop sentence\n",
    "            new_sentence = sentence[:sequence_length]\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_data(sentences, labels=None, vocabulary=None):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data_old():\n",
    "#     \"\"\"\n",
    "#     Loads and preprocessed data for the dataset.\n",
    "#     Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "#     \"\"\"\n",
    "#     # Load and preprocess data\n",
    "#     sentences, labels = load_data_and_labels()\n",
    "#     sentences_padded = pad_sentences(sentences)\n",
    "#     vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "#     x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    \n",
    "#     return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    train_sentences, train_labels = load_data_and_labels(pos_file='cnn/df_train.pos', neg_file='cnn/df_train.neg')\n",
    "    val_sentences, val_labels = load_data_and_labels(pos_file='cnn/df_val.pos', neg_file='cnn/df_val.neg')\n",
    "    test_sentences, test_labels = load_data_and_labels(pos_file='cnn/df_test.pos', neg_file='cnn/df_test.neg')\n",
    "    unlab_sentences = load_data_and_labels(pos_file='cnn/df_unlab.nolabels')\n",
    "    \n",
    "    # sentences to build vocab\n",
    "    all_sentences = train_sentences+val_sentences+test_sentences+unlab_sentences\n",
    "    \n",
    "    \n",
    "    train_sentences_padded = pad_sentences(train_sentences)\n",
    "    val_sentences_padded = pad_sentences(val_sentences)\n",
    "    test_sentences_padded = pad_sentences(test_sentences)\n",
    "    unlab_sentences_padded = pad_sentences(unlab_sentences)\n",
    "    \n",
    "    \n",
    "    #probably need to create all vocab and then split to val, test etc\n",
    "    vocabulary, vocabulary_inv = build_vocab(pad_sentences(all_sentences))\n",
    "    \n",
    "    x_train, y_train = build_input_data(train_sentences_padded, train_labels, vocabulary)    \n",
    "    x_val, y_val = build_input_data(val_sentences_padded, val_labels, vocabulary)\n",
    "    x_test, y_test = build_input_data(test_sentences_padded, test_labels, vocabulary)\n",
    "    x_unlab, _ = build_input_data(unlab_sentences_padded, val_labels, vocabulary) #passing dummy labels here, not using them afterwards\n",
    "    \n",
    "    \n",
    "    return [x_train, y_train, x_val, y_val, x_test, y_test, x_unlab, vocabulary, vocabulary_inv ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** model.py **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "# x, y, vocabulary, vocabulary_inv = load_data()\n",
    "\n",
    "# Define max sequence length / or infer from training set (creates problem in new vocab though)\n",
    "sequence_length = 64 # 56\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_unlab, vocabulary, vocabulary_inv = load_data()\n",
    "# sequence_length = X_train.shape[1] # 56\n",
    "\n",
    "vocabulary_size = len(vocabulary_inv) # 18765\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Balance training set (either with class weights or undersampling) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_weights = dict(pd.Series(map(lambda x: x[0],y_train)).value_counts()) # inverse class_count\n",
    "# sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights = {0: 3302., 1: 9685.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MLutils import balance_candidates\n",
    "\n",
    "# X_train_us, y_train_us = balance_candidates(X_train, list(map(lambda x: x[0],y_train)))\n",
    "# y_train_us = np.array(map(lambda x: np.array([1,0] if x==0 else np.array([0,1])),y_train_us))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** train params **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # original: 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 8\n",
    "batch_size = 32\n",
    "sequence_length = X_train.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define F1 (+) as metric **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestCallback(Callback):\n",
    "#     def __init__(self, test_data):\n",
    "#         self.test_data = test_data\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         x, y = self.test_data\n",
    "#         loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "#         print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** model building **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n"
     ]
    }
   ],
   "source": [
    "# this returns a tensor\n",
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip for now - try class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6604"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "y_train_bin = list(map(lambda x: x[1],y_train))\n",
    "X_train_us, y_train_us = rus.fit_sample(X_train,y_train_bin)\n",
    "\n",
    "\n",
    "y_train_us = np.array(map(lambda x: [0,1] if x==1 else [1,0],y_train_us)) #convert back to logits\n",
    "\n",
    "len(X_train_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6604, 64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_us.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6604, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_us.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with undersampling \n",
    "\n",
    "# model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy', f1])\n",
    "# print(\"Traning Model...\")\n",
    "# model.fit(X_train_us, y_train_us, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], \n",
    "#          validation_data=(X_val, y_val),\n",
    "#          )  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use val_labels for y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix F1 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, val_data, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.val_data = val_data\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        val_predict = (np.asarray(self.model.predict(self.val_data[0]))).round()\n",
    "        val_predict = map(lambda x: int(x[1]),val_predict)\n",
    "        val_targ = self.val_data[1]\n",
    "        val_targ = map(lambda x: x[1],val_targ)\n",
    "        \n",
    "#         _val_f1 = f1_score(val_targ, val_predict,)\n",
    "#         _val_recall = recall_score(val_targ, val_predict)\n",
    "#         _val_precision = precision_score(val_targ, val_predict)\n",
    "        \n",
    "        _val_precision,_val_recall,_val_f1, support = precision_recall_fscore_support(val_targ, val_predict)\n",
    "        \n",
    "        #due to the fact that the positive class is [0,1], we discard the first elemts (correspond to neg class)\n",
    "        _val_f1 = _val_f1[1]\n",
    "        _val_precision = _val_precision[1]\n",
    "        _val_recall = _val_recall[1]\n",
    "        \n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        \n",
    "        print 'val_targ[:10]',val_targ[:10],'\\n', 'val_predict[:10]',val_predict[:10],'\\n'\n",
    "        \n",
    "        print '_val_f1',_val_f1\n",
    "        print 'support',support\n",
    "        \n",
    "        \n",
    "        print '\\n— val_f1: %.2f — val_precision: %.2f — val_recall %.2f\\n' %(_val_f1, _val_precision, _val_recall)\n",
    "#         print _val_f1\n",
    "#         print _val_precision\n",
    "#         print _val_recall\n",
    "        print ''\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "Train on 6604 samples, validate on 3230 samples\n",
      "Epoch 1/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.6803 - acc: 0.5747 - val_loss: 0.6431 - val_acc: 0.7368\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [0, 1, 1, 0, 1, 0, 0, 0, 0, 0] \n",
      "\n",
      "_val_f1 0.424898511502\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.42 — val_precision: 0.41 — val_recall 0.45\n",
      "\n",
      "\n",
      "Epoch 2/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.6347 - acc: 0.6938 - val_loss: 0.6214 - val_acc: 0.6988\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [1, 1, 1, 0, 1, 1, 1, 0, 0, 1] \n",
      "\n",
      "_val_f1 0.510809451986\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.51 — val_precision: 0.40 — val_recall 0.72\n",
      "\n",
      "\n",
      "Epoch 3/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.5613 - acc: 0.7583 - val_loss: 0.5496 - val_acc: 0.7508\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [1, 1, 1, 0, 1, 1, 1, 0, 1, 1] \n",
      "\n",
      "_val_f1 0.557449147883\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.56 — val_precision: 0.46 — val_recall 0.72\n",
      "\n",
      "\n",
      "Epoch 4/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.4761 - acc: 0.8039 - val_loss: 0.4766 - val_acc: 0.7774\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [0, 1, 1, 0, 1, 1, 1, 0, 1, 1] \n",
      "\n",
      "_val_f1 0.586067933218\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.59 — val_precision: 0.49 — val_recall 0.72\n",
      "\n",
      "\n",
      "Epoch 5/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.4064 - acc: 0.8328 - val_loss: 0.4598 - val_acc: 0.7830\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [0, 1, 1, 0, 1, 1, 1, 0, 1, 1] \n",
      "\n",
      "_val_f1 0.60816098379\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.61 — val_precision: 0.50 — val_recall 0.77\n",
      "\n",
      "\n",
      "Epoch 6/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.3520 - acc: 0.8592 - val_loss: 0.4209 - val_acc: 0.8003\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [0, 1, 1, 1, 1, 1, 1, 0, 1, 1] \n",
      "\n",
      "_val_f1 0.621256605989\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.62 — val_precision: 0.53 — val_recall 0.75\n",
      "\n",
      "\n",
      "Epoch 7/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.3097 - acc: 0.8810 - val_loss: 0.3975 - val_acc: 0.8003\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [0, 1, 1, 1, 1, 0, 1, 0, 1, 1] \n",
      "\n",
      "_val_f1 0.610271903323\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.61 — val_precision: 0.53 — val_recall 0.72\n",
      "\n",
      "\n",
      "Epoch 8/8\n",
      "6604/6604 [==============================] - 16s 2ms/step - loss: 0.2745 - acc: 0.8992 - val_loss: 0.3893 - val_acc: 0.8040\n",
      "val_targ[:10] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
      "val_predict[:10] [0, 1, 1, 1, 1, 0, 1, 0, 1, 1] \n",
      "\n",
      "_val_f1 0.614729153987\n",
      "support [2525  705]\n",
      "\n",
      "— val_f1: 0.61 — val_precision: 0.54 — val_recall 0.72\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb5eff2210>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = Metrics((X_val,y_val))\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "model.fit(X_train_us, y_train_us, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[metrics], \n",
    "         validation_data=(X_val, y_val),\n",
    "         )  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** quick check if keras metric is ok **\n",
    "\n",
    "## TODO: it is not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #evaluation with 0s\n",
    "\n",
    "# logits = model.predict(X_val)\n",
    "\n",
    "# y_val_gold = map(lambda x: x[1], y_val)\n",
    "# y_val_pred_bin = map(lambda x: 1 if x[1]>=0.5 else 0,logits) #if logit of 1 class >=0.5\n",
    "# class_report = report_to_df(classification_report(y_val_gold, y_val_pred_bin))\n",
    "\n",
    "# print class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from MLutils import report_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to convert to/from Snorkels -1,1 class label lists\n",
    "\n",
    "def logits_to_neg_labels(logit_array):\n",
    "    \"\"\"Input: [[0.93, 0.07],\n",
    "       [0,1],\n",
    "       [0,1]]\n",
    "       Output: [-1,1,1]\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: 1 if x[1]>=0.5 else -1, logit_array))\n",
    "\n",
    "def logits_to_bin_labels(logit_array):\n",
    "    \"\"\"Input: [[1, 0],\n",
    "           [0,1],\n",
    "           [0,1]]\n",
    "           Output: [0,1,1]\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: 1 if x[1]>=0.5 else 0, logit_array))\n",
    "\n",
    "def get_positive_logit(logit_array):\n",
    "    \"\"\"Input: [[1, 0],\n",
    "           [0,1],\n",
    "           [0,1]]\n",
    "           Output: [0,1,1]\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: x[1], logit_array))\n",
    "\n",
    "\n",
    "\n",
    "def bin_to_neg_labels(label_list):\n",
    "    return [-1 if x==0 else 1 for x in label_list]\n",
    "\n",
    "def neg_to_bin_labels(label_list):\n",
    "    return [0 if x==-1 else 1 for x in label_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_report_from_dicts(true_dict, pred_dict):\n",
    "    ids = true_dict.keys()\n",
    "    true_list = map(lambda x: true_dict[x],ids)\n",
    "    pred_list = map(lambda x: pred_dict[x],ids)\n",
    "    return classification_report(true_list, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision  recall  f1-score  support\n",
      "Classes                                        \n",
      "-1              0.91    0.83      0.87     2525\n",
      "1               0.54    0.72      0.61      705\n",
      "avg/total       0.83    0.80      0.81     3230\n",
      "           precision  recall  f1-score  support\n",
      "Classes                                        \n",
      "-1              0.91    0.83      0.87     2525\n",
      "1               0.54    0.72      0.61      705\n",
      "avg/total       0.83    0.80      0.81     3230\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "\n",
    "# label_*_binary -> list of -1, 1\n",
    "# label_*_prob_dict -> {40197: 0.96, 40198: 0.03, ... } (close to 1 -> True/Relevant class )\n",
    "# NOTE: positive_labels = [[0, 1] for _ in positive_examples]\n",
    "\n",
    "\n",
    "# pass predictions as dict & binary list\n",
    "logits = model.predict(X_val)\n",
    "# y_val_pred = dict(zip(val_ids, list(map(lambda x: x[1], logits)))) #set the probability for the -1 class\n",
    "\n",
    "y_val_pred_pos_logit = dict(zip(val_ids, get_positive_logit(logits)))\n",
    "y_val_pred_bin = dict(zip(val_ids, logits_to_bin_labels(logits))) #only used for evaluation\n",
    "y_val_pred_neg = dict(zip(val_ids, logits_to_neg_labels(logits))) #only used for evaluation\n",
    "\n",
    "\n",
    "# evaluation on validation set\n",
    "y_val_gold = dict(df_val.label)\n",
    "y_test_gold = dict(df_test.label)\n",
    "\n",
    "\n",
    "# create classification report based on validation set values\n",
    "class_report = report_to_df(\n",
    "    classif_report_from_dicts(y_val_gold, y_val_pred_neg)\n",
    "                         )\n",
    "print class_report\n",
    "\n",
    "#print another class_report\n",
    "\n",
    "# this is correct - because the logits are ordered by + and -\n",
    "print report_to_df(\n",
    "    classification_report(\n",
    "        logits_to_neg_labels(y_val),\n",
    "        logits_to_neg_labels(logits)\n",
    "    )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "logits = model.predict(X_test)\n",
    "y_test_pred_pos_logit = dict(zip(test_ids, get_positive_logit(logits)))\n",
    "y_test_pred_bin = dict(zip(val_ids, logits_to_bin_labels(logits))) #only used for evaluation\n",
    "y_test_pred_neg = dict(zip(val_ids, logits_to_neg_labels(logits))) #only used for evaluation\n",
    "\n",
    "logits = model.predict(X_unlab)\n",
    "y_unlab_pred_pos_logit = dict(zip(unlab_ids, get_positive_logit(logits)))\n",
    "\n",
    "\n",
    "\n",
    "# To delete\n",
    "# logits = model.predict(X_test)\n",
    "# y_test_pred = dict(zip(test_ids, list(map(lambda x: x[1], logits)))) #set the probability for the -1 class\n",
    "# y_test_pred_bin = dict(zip(test_ids, list(map(lambda x: int(x[1]>=0.5), logits)))) #only used for evaluation\n",
    "\n",
    "\n",
    "# logits = model.predict(X_unlab)\n",
    "# y_unlab_pred = dict(zip(unlab_ids, list(map(lambda x: x[1], logits)))) #set the probability for the -1 class\n",
    "# y_unlab_pred_bin = dict(zip(unlab_ids, list(map(lambda x: int(x[1]>=0.5), logits)))) #only used for evaluation\n",
    "\n",
    "\n",
    "\n",
    "results_dict['CNN'] = {\"label_val_prob+\" : y_val_pred_pos_logit,\n",
    "                       \"label_test_prob+\" : y_test_pred_pos_logit,\n",
    "                       \"label_unlab_prob+\" : y_unlab_pred_pos_logit,\n",
    "                        \"classification_report\": class_report,\n",
    "                        \"f1+\": class_report.loc['1','f1-score']\n",
    "                           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_val_prob+',\n",
       " 'label_test_prob+',\n",
       " 'classification_report',\n",
       " 'label_unlab_prob+',\n",
       " 'f1+']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict['CNN'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ml_predictions/CNN_test.pkl', 'wb') as f:\n",
    "    pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snorkel27clone]",
   "language": "python",
   "name": "conda-env-snorkel27clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
