{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up variables & DB connection for experiment:\n",
      "\n",
      "*******************\n",
      "_exp3\n",
      "*******************\n",
      "\n",
      "Snorkel session connected to:  postgres:///snorkel_exp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonis/anaconda2/envs/snorkel27clone/lib/python2.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "experiment_name = '25similar'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%run init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from snorkel.annotations import save_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonis/anaconda2/envs/snorkel27clone/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/antonis/anaconda2/envs/snorkel27clone/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "import pickle,glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MLutils import cohen_kappa_score, plot_marginals_histogram, neg_to_bin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLutils import diversity_heatmap, merge_pickles_pred_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move in LSTM training section\n",
    "\n",
    "# needed to train the LSTM here\n",
    "train = session.query(REGULATOR).filter(REGULATOR.split == 0).order_by(REGULATOR.id).all()\n",
    "dev = session.query(REGULATOR).filter(REGULATOR.split == 1).order_by(REGULATOR.id).all()\n",
    "test = session.query(REGULATOR).filter(REGULATOR.split == 2).order_by(REGULATOR.id).all()\n",
    "unlab = session.query(REGULATOR).filter(REGULATOR.split == 3).order_by(REGULATOR.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_annotator(c):\n",
    "    \"\"\"A generator over the different (worker_id, label_id) pairs for a Tweet.\"\"\"\n",
    "    for model in results_dict.keys():\n",
    "        try:\n",
    "            yield model, results_dict[model]['label_unlab'][c.id]\n",
    "            \n",
    "        except:\n",
    "            try:\n",
    "                yield model, results_dict[model]['label_val'][c.id]\n",
    "            except:\n",
    "                yield model, results_dict[model]['label_test'][c.id]\n",
    "\n",
    "labeler = LabelAnnotator(label_generator=signal_annotator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve results and pick models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from MLutils import diversity_matrix, reduce_results_dict, balance_candidates, sample_from_csr, majority_vote, majority_vote_score, average_vote, error_analysis\n",
    "from utils import check_class_imbalance\n",
    "from itertools import combinations\n",
    "from snorkel.learning import RandomSearch, GenerativeModel, GridSearch\n",
    "from snorkel.learning.structure import DependencySelector\n",
    "from snorkel.learning import reRNN\n",
    "\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickles/_exp3/base_learner_predictions/KMeansPeaks/10clusters.pickle',\n",
       " 'pickles/_exp3/base_learner_predictions/KMeansPeaks/13clusters.pickle',\n",
       " 'pickles/_exp3/base_learner_predictions/KMeansPeaks/15clusters.pickle',\n",
       " 'pickles/_exp3/base_learner_predictions/KMeansPeaks/20clusters.pickle',\n",
       " 'pickles/_exp3/base_learner_predictions/KMeansPeaks/21clusters.pickle',\n",
       " 'pickles/_exp3/base_learner_predictions/KMeansPeaks/5clusters.pickle']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = glob.glob(path_base_learners+\"/KMeansPeaks/*.pickle\")\n",
    "l.sort()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.DataFrame(index=l)\n",
    "test_scores['n_classif'] = list(map(lambda x: x.split('/')[-1] ,test_scores.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_file = l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = gen_model.score(L_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5658274865868758, 0.6910282258064516, 0.6221919673247107)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['results_dict,lemmas,RuS,CV_,bin_,minFreq=5,_ngrams=(0, 3)_SVC_rbf_C=75',\n",
       " 'results_dict,ShortDepPath,lemmas,RuS,TfIdf_,minFreq=5,_ngrams=(0, 3),LSA200_SVC_linear',\n",
       " 'results_dict,trim=5,lemmas,RuS,TfIdf_,minFreq=5,_ngrams=(0, 3)_SVC_linear',\n",
       " 'results_dict,trim=5,lemmas,RuS,TfIdf_,minFreq=5,_stopw=english,_ngrams=(1, 1),LSA200_SVC_linear',\n",
       " 'results_dict,trim=5,lemmas,RuS,TfIdf_,minFreq=5,_ngrams=(0, 3)_RandomForestClassifier',\n",
       " 'biLSTM_biLSTM_lr=0.001',\n",
       " 'results_dict,trim=0,lemmas,RuS,TfIdf_,minFreq=5,_ngrams=(0, 3),LSA200_SVC_linear',\n",
       " 'results_dict,lemmas,RuS,TfIdf_,minFreq=5,_stopw=english,_ngrams=(1, 1),LSA200_SVC_rbf_C=75',\n",
       " 'results_dict,ShortDepPath,lemmas,RuS,CV_,bin_,minFreq=5,_stopw=english,_ngrams=(1, 1),LSA200_LogisticRegression',\n",
       " 'results_dict,trim=0,lemmas,RuS,TfIdf_,minFreq=5,_stopw=english,_ngrams=(1, 1),LSA200_SVC_rbf_C=250']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for result_dict_file in l[1:]:\n",
    "    with open(result_dict_file, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    print 'Loaded:\\n',\n",
    "    print results_dict.keys()\n",
    "\n",
    "    selected_models = results_dict.keys()\n",
    "    selected_models.sort()\n",
    "\n",
    "    # Load predicted labels from classifiers\n",
    "\n",
    "\n",
    "    # convert labels to -1,1....\n",
    "    for model in selected_models:\n",
    "        results_dict[model]['label_unlab'] = dict(zip(results_dict[model]['label_unlab_prob+'].keys(),(np.array(results_dict[model]['label_unlab_prob+'].values())>=0.5)*2-1))\n",
    "        results_dict[model]['label_val'] = dict(zip(results_dict[model]['label_val_prob+'].keys(),(np.array(results_dict[model]['label_val_prob+'].values())>=0.5)*2-1))\n",
    "        results_dict[model]['label_test'] = dict(zip(results_dict[model]['label_test_prob+'].keys(),(np.array(results_dict[model]['label_test_prob+'].values())>=0.5)*2-1))\n",
    "\n",
    "\n",
    "    ## Port results into snorkel\n",
    "\n",
    "    ### Apply labeler / load label matrix from pickle\n",
    "\n",
    "    #regenerate L_unlab - Takes about 1h for 12 voters (100% coverage)\n",
    "\n",
    "    L_unlab = labeler.apply(split=3, parallelism=6)\n",
    "\n",
    "    # load validation & test set & teir labels\n",
    "    L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "    L_dev = labeler.apply_existing(split=1, parallelism=6)\n",
    "\n",
    "    L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "    L_test = labeler.apply_existing(split=2, parallelism=6)\n",
    "\n",
    "    # load unlabeled set labels (for exp3)\n",
    "    L_gold_unlab = load_gold_labels(session, annotator_name='gold', split=3)\n",
    "\n",
    "    ### Check some label statistics\n",
    "\n",
    "\n",
    "\n",
    "    #** Majority voting **\n",
    "    error_analysis(L_dev, L_gold_dev, majority_voting=True, save_name=result_dict_file+'MV.png')\n",
    "    test_scores.loc[result_dict_file, 'MV_P'] , test_scores.loc[result_dict_file, 'MV_R'] , test_scores.loc[result_dict_file, 'MV_F1']  = majority_vote_score(L_dev, L_gold_dev)\n",
    "\n",
    "\n",
    "    #** Average voting **\n",
    "    error_analysis(L_dev, L_gold_dev, average_voting=True,  save_name=result_dict_file+'AvgV.png')\n",
    "    \n",
    "    \n",
    "    # Denoising part\n",
    "\n",
    "    ## Find dependencies between LFs\n",
    "    ds = DependencySelector()\n",
    "\n",
    "    # Generate all possible dependencies with threshold from 0 to 0.15\n",
    "    all_deps = [ds.select(L_unlab, threshold=thresh) for thresh in np.array(range(0.1,15.1,2.5))/100.]\n",
    "    print list(map(lambda x: len(x) ,all_deps))\n",
    "    all_deps = list(np.unique(all_deps)) # delete duplicates for gridsearch\n",
    "    print list(map(lambda x: len(x) ,all_deps))\n",
    "\n",
    "    print \"Maximum nr of dependencies: %i\" %len(list(combinations(L_dev.col_index.values(),2)))\n",
    "\n",
    "    # GM training\n",
    "\n",
    "\n",
    "\n",
    "    # without HyperParam search\n",
    "    gen_model = GenerativeModel()\n",
    "    gen_model.train(L_unlab)\n",
    "    test_scores.loc[result_dict_file, 'GM_default_P'], test_scores.loc[result_dict_file, 'GM_default_R'], test_scores.loc[result_dict_file, 'GM_default_F1'] = majority_vote_score(L_test, L_gold_test)\n",
    "\n",
    "\n",
    "    #** Hyperparameter search **\n",
    "\n",
    "    param_ranges = { # parameters for the train function of the GenerativeModel\n",
    "        'step_size' : [10./L_unlab.shape[0],\n",
    "                      5./L_unlab.shape[0],\n",
    "                       15./L_unlab.shape[0]\n",
    "                      ],\n",
    "        'decay' : [0.99],\n",
    "        'epochs' : [50],\n",
    "        'reg_param': [1e-1,1e-2,1e-3],\n",
    "        'deps': all_deps\n",
    "    }\n",
    "\n",
    "    model_class_params = {  # parameters for the GenerativeModel\n",
    "        'lf_propensity': [True]\n",
    "    }\n",
    "\n",
    "    searcher = RandomSearch(GenerativeModel, param_ranges,  L_unlab, model_class_params = model_class_params , n=24 )\n",
    "\n",
    "    gen_model, run_stats = searcher.fit(L_dev, L_gold_dev, n_threads=6 )\n",
    "    \n",
    "    test_scores.loc[result_dict_file, 'GM_hyperp_P'], test_scores.loc[result_dict_file, 'GM_hyperp_R'], test_scores.loc[result_dict_file, 'GM_hyperp_F1'] = gen_model.score(L_test, L_gold_test)\n",
    "\n",
    "    # Save marginals and proceed to LSTM training\n",
    "\n",
    "    unlab_marginals_gen = gen_model.marginals(L_unlab)\n",
    "    unlab_marginals_avg = average_vote(L_unlab)\n",
    "    unlab_marginals_maj = majority_vote(L_unlab)\n",
    "\n",
    "    \n",
    "    # Discriminative model learning\n",
    "    ## Without GS\n",
    "\n",
    "    plot_marginals_histogram(unlab_marginals_gen, \n",
    "                             title = 'Histogram of marginals' ,\n",
    "                            bins = 20)\n",
    "\n",
    "    train_kwargs = {\n",
    "        'lr':         0.01,\n",
    "        'dim':        100,\n",
    "        'n_epochs':   25,\n",
    "        'dropout':    0.5,\n",
    "        'rebalance':  True,\n",
    "        'print_freq': 1,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "\n",
    "    lstm = reRNN(seed=1701, n_threads=6,)\n",
    "    lstm.train(unlab, unlab_marginals_gen , X_dev=dev, Y_dev=L_gold_dev, dev_ckpt_delay=0, **train_kwargs)\n",
    "\n",
    "    test_scores.loc[result_dict_file, 'LSTM + GM_P'], test_scores.loc[result_dict_file, 'LSTM + GM_R'], test_scores.loc[result_dict_file, 'LSTM + GM_F1'] = lstm.score(test, L_gold_test)\n",
    "\n",
    "    #** with averaging marginals **\n",
    "\n",
    "    train_kwargs = {\n",
    "        'lr':         0.01,\n",
    "        'dim':        100,\n",
    "        'n_epochs':   25,\n",
    "        'dropout':    0.5,\n",
    "        'rebalance':  True,\n",
    "        'print_freq': 1,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "\n",
    "    lstm = reRNN(seed=1701, n_threads=6,)\n",
    "    lstm.train(unlab, unlab_marginals_avg , X_dev=dev, Y_dev=L_gold_dev,  dev_ckpt_delay=0, **train_kwargs)\n",
    "\n",
    "    test_scores.loc[result_dict_file, 'LSTM + AvgV_P'], test_scores.loc[result_dict_file, 'LSTM + AvgV_R'], test_scores.loc[result_dict_file, 'LSTM + AvgV F1'] = lstm.score(test, L_gold_test)\n",
    "\n",
    "\n",
    "    train_kwargs = {\n",
    "        'lr':         0.01,\n",
    "        'dim':        100,\n",
    "        'n_epochs':   25,\n",
    "        'dropout':    0.5,\n",
    "        'rebalance':  True,\n",
    "        'print_freq': 1,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "\n",
    "    lstm = reRNN(seed=1701, n_threads=6,)\n",
    "    lstm.train(unlab, unlab_marginals_maj , X_dev=dev, Y_dev=L_gold_dev, dev_ckpt_delay=0, **train_kwargs)\n",
    "\n",
    "    test_scores.loc[result_dict_file, 'LSTM + MV_P'], test_scores.loc[result_dict_file, 'LSTM + MV_R'], test_scores.loc[result_dict_file, 'LSTM + MV_F1'] = lstm.score(test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MLutils import balance_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snorkel.annotations import load_marginals\n",
    "# train_labels = np.array([0 if train[x].gold_labels[0].value ==-1 else 1 for x in range(len(train))]) # load train labels to mix in LSTM\n",
    "# unlab_marginals = load_marginals(session, split=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #manually balance candidates\n",
    "# train_us, train_labels_us = balance_candidates(train,train_labels)\n",
    "# unlab_us, unlab_marginals_us = balance_candidates(unlab,unlab_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_unlab_marginals = np.append(train_labels_us, unlab_marginals_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(train_unlab_marginals, bins=20)\n",
    "# plt.title('Histogram of marginals (candidates undersampled before GM) + GS ')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_kwargs = {\n",
    "#     'lr':         0.01,\n",
    "#     'dim':        100,\n",
    "#     'n_epochs':   25,\n",
    "#     'dropout':    0.5,\n",
    "#     'rebalance':  False,\n",
    "#     'print_freq': 1,\n",
    "#     'batch_size': 64\n",
    "# }\n",
    "\n",
    "# lstm = reRNN(seed=1701, n_threads=6)\n",
    "# lstm.train(train_us+unlab_us, train_unlab_marginals, X_dev=dev, Y_dev=L_gold_dev, dev_ckpt_delay=0, **train_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "# tp, fp, tn, fn = lstm.error_analysis(session, dev, L_gold_dev, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "# with open(result_dict_file+\".txt\", \"a\") as f:\n",
    "#     f.write(unicode(\"LSTM + GM (val set) trained on GS+US:\\n\"))\n",
    "#     f.write(cap.stdout)\n",
    "#     f.write(unicode('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "# tp, fp, tn, fn = lstm.error_analysis(session, test, L_gold_test, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(result_dict_file+\".txt\", \"a\") as f:\n",
    "#     f.write(unicode(\"LSTM + GM (test set) trained on GS+US:\\n\"))\n",
    "#     f.write(cap.stdout)\n",
    "#     f.write(unicode('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_scores.loc[result_dict_file, 'LSTM + GM (GS+US)'] = lstm.score(test, L_gold_test)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'newlist' object has no attribute 'to_latex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-645a73af3d91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_dict_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_latex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'newlist' object has no attribute 'to_latex'"
     ]
    }
   ],
   "source": [
    "with open(result_dict_file+\".txt\", \"a\") as f:\n",
    "    f.write(unicode(test_scores.to_latex()))\n",
    "    f.write(unicode('\\n\\n'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snorkel27clone]",
   "language": "python",
   "name": "conda-env-snorkel27clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
